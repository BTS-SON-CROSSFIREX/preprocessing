{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa34b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import preprocessor as p\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia=SIA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a385eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 내 사용\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"&gt;\" : \"is greater than\", \"&lt;\" : \"is less than\", \"&nbsp;\": \" \", \"&amp;\" : \"and\", \"&quot;\" : '\"'}\n",
    "\n",
    "# 영어 사진\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def preprocessing(data) :\n",
    "    data = re.sub('’|‘|´|`', '\\'', data)\n",
    "    data = re.sub('\\\\n', ' ', data)\n",
    "    # lowering\n",
    "    data = data.lower()\n",
    "    # normalize abbreviation\n",
    "    data = ' '.join([contractions[t] if t in contractions else t for t in data.split(\" \")])\n",
    "    # clean corpus to remove emoji and URL\n",
    "    data = p.clean(data)\n",
    "    # remove words shorter than length 1\n",
    "    data = ' '.join(word for word in data.split())\n",
    "    return data\n",
    "\n",
    "def only_alpha(data) :\n",
    "    # remove numbers and punctuations\n",
    "    data = re.sub(r'[^A-Za-z ]', ' ', data)\n",
    "    # remove duplicated alphabets\n",
    "    data = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', data)\n",
    "    # remove words shorter than length 1\n",
    "    data = ' '.join(word for word in data.split() if len(word) > 1)\n",
    "    return data\n",
    "\n",
    "def approx_english_detection(line) :\n",
    "    total_words = line.split()\n",
    "    eng_num = 0;\n",
    "    for w in total_words: \n",
    "        if w in words : eng_num += 1\n",
    "    if(eng_num / len(total_words) >= 0.5) : return 'en'\n",
    "    return 'other'\n",
    "\n",
    "def get_score(line) :\n",
    "    score = sia.polarity_scores(line)['compound']\n",
    "    if score > 0.2 : return 1\n",
    "    elif score < -0.2 : return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d290d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword : pubg\n"
     ]
    }
   ],
   "source": [
    "keyword = input(\"keyword : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7be519ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file : pubg_2022-04-13_2022-05-16_tweetdata\n"
     ]
    }
   ],
   "source": [
    "filename = input(\"file : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aa10c0aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82106\\AppData\\Local\\Temp/ipykernel_19696/542986543.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Text'] = result['Text'].apply(lambda x : only_alpha(x))\n",
      "C:\\Users\\82106\\AppData\\Local\\Temp/ipykernel_19696/542986543.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['Clean'] = result['Text'].apply(lambda x : ' '.join(word for word in x.split() if not word in stop_words if len(word) > 1))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./corpus/\" + keyword + '/' + filename + \".csv\", encoding='UTF-8')\n",
    "df = df[['Datetime','Text']]\n",
    "\n",
    "# 전처리\n",
    "df['Text'] = df['Text'].apply(lambda x : preprocessing(x))\n",
    "df_temp = df[df['Text'] == ''].index\n",
    "df = df.drop(df_temp)\n",
    "\n",
    "# 영어 사진에 키워드 추가\n",
    "words.add(keyword)\n",
    "\n",
    "df['approx'] = df['Text'].apply(lambda x : approx_english_detection(x))\n",
    "result = df.loc[df['approx'] == 'en']\n",
    "\n",
    "result['Text'] = result['Text'].apply(lambda x : only_alpha(x))\n",
    "\n",
    "# 불용어 처리\n",
    "stop_words = set(stopwords.words('english')) \n",
    "result['Clean'] = result['Text'].apply(lambda x : ' '.join(word for word in x.split() if not word in stop_words if len(word) > 1))\n",
    "\n",
    "# 없어진 문장 처리\n",
    "result_temp = result[result['Clean'] == ''].index\n",
    "result = result.drop(result_temp)\n",
    "\n",
    "result = result[['Datetime', 'Text', 'Clean']].set_index('Datetime')\n",
    "result.to_csv('./preprocessed/' + keyword + '/' + filename + '_processed_revised.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43a33b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Label'] = result['Text'].apply(lambda x : get_score(x))\n",
    "result.to_csv('./labeled/' + keyword + '/revised/' + filename + '_labeled_revised.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4af93f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = result[result['Label'] == 1]\n",
    "neutral = result[result['Label'] == 0]\n",
    "negative = result[result['Label'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c2403768",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive.to_csv(\"./labeled/\"+keyword+'/revised/'+filename+'_positive_revised.csv')\n",
    "negative.to_csv(\"./labeled/\"+keyword+'/revised/'+filename+'_negative_revised.csv')\n",
    "neutral.to_csv('./labeled/' +keyword+'/revised/'+filename+'_neutral_revised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ccc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
